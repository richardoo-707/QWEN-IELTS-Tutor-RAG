import os
import torch
from typing import Annotated, List, Dict, TypedDict
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- ç¤¾åŒºç»„ä»¶ ---
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
# BM25 å¿…é¡»ä» community å¯¼å…¥
from langchain_community.retrievers import BM25Retriever
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# --- æ ¸å¿ƒç»„ä»¶ (æ£€ç´¢å™¨) ---
try:
    # å°è¯•ä»ä¸»åŒ…å¯¼å…¥ (æ ‡å‡†åšæ³•)
    from langchain.retrievers import EnsembleRetriever
    from langchain.retrievers import ContextualCompressionRetriever
    from langchain.retrievers.document_compressors import CrossEncoderReranker
except ImportError:
    # å¤‡é€‰æ–¹æ¡ˆï¼šå°è¯•ä» community å¯¼å…¥ (å…¼å®¹æ—§ç‰ˆ/é”™ç‰ˆ)
    print("âš ï¸ æ­£åœ¨å°è¯•ä» community å¯¼å…¥æ£€ç´¢å™¨...")
    from langchain_community.retrievers import EnsembleRetriever
    from langchain_community.retrievers import ContextualCompressionRetriever
    from langchain.retrievers.document_compressors import CrossEncoderReranker

# --- å›¾é€»è¾‘ ---
from langgraph.graph import END, StateGraph
# ================= é…ç½® =================
DB_PATH = "./vector_db"
EMBEDDING_MODEL = "moka-ai/m3e-base"
RERANK_MODEL = "BAAI/bge-reranker-base"
LLM_MODEL = "qwen2.5:7b"

print("âš™ï¸  1. åˆå§‹åŒ– Embeddings (CPU)...")
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={'device': 'cpu'}
)

print(f"ğŸ”— 2. è¿æ¥å‘é‡åº“ {DB_PATH}...")
if not os.path.exists(DB_PATH):
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°å‘é‡åº“ï¼Œè¯·å…ˆè¿è¡Œ build.pyï¼")
    exit()

vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

print("â³ 3. æ„å»ºæ··åˆæ£€ç´¢ (BM25 + Vector)...")
try:
    # è·å–æ‰€æœ‰æ–‡æ¡£ç”¨äºæ„å»º BM25 ç´¢å¼•
    db_data = vectordb.get()
    all_docs = db_data["documents"]
    metadatas = db_data["metadatas"]

    if not all_docs:
        print("âŒ å‘é‡åº“æ˜¯ç©ºçš„ï¼è¯·æ£€æŸ¥ data æ–‡ä»¶å¤¹å¹¶é‡æ–°è¿è¡Œ build.py")
        exit()

    doc_objects = [Document(page_content=t, metadata=m) for t, m in zip(all_docs, metadatas)]

    # BM25 æ£€ç´¢å™¨
    bm25_retriever = BM25Retriever.from_documents(doc_objects)
    bm25_retriever.k = 5

    # å‘é‡æ£€ç´¢å™¨
    vector_retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    # æ··åˆæ£€ç´¢å™¨
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.4, 0.6]
    )
except Exception as e:
    print(f"âŒ æ„å»ºæ£€ç´¢å™¨å¤±è´¥: {e}")
    exit()

print("âš–ï¸  4. åŠ è½½é‡æ’åºæ¨¡å‹ (Reranker)...")
try:
    rerank_model = HuggingFaceCrossEncoder(model_name=RERANK_MODEL, model_kwargs={'device': 'cpu'})
    compressor = CrossEncoderReranker(model=rerank_model, top_n=3)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=ensemble_retriever
    )
except Exception as e:
    print(f"âš ï¸  é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜): {e}")
    print("âš ï¸  å°†é™çº§ä½¿ç”¨æ™®é€šæ··åˆæ£€ç´¢...")
    compression_retriever = ensemble_retriever

print(f"ğŸ¤– 5. è¿æ¥ Ollama ({LLM_MODEL})...")
llm = ChatOllama(model=LLM_MODEL, temperature=0)


# === å®šä¹‰çŠ¶æ€ (State) ===
class GraphState(TypedDict):
    question: str
    generation: str
    documents: List[str]
    retry_count: int


# === å®šä¹‰èŠ‚ç‚¹ (Nodes) ===
def retrieve(state):
    print(f"\nğŸ” [Step 1: æ£€ç´¢] é—®é¢˜: {state['question']}")
    question = state["question"]
    docs = compression_retriever.invoke(question)
    doc_texts = [d.page_content for d in docs]
    print(f"   - æ£€ç´¢åˆ° {len(doc_texts)} æ¡ç›¸å…³ç‰‡æ®µ")
    return {"documents": doc_texts, "question": question}


def generate(state):
    print("âœï¸  [Step 2: ç”Ÿæˆ] æ¨¡å‹æ­£åœ¨æ’°å†™å›ç­”...")
    question = state["question"]
    documents = state["documents"]

    context = "\n\n".join(documents) if documents else "æ— ç›¸å…³èµ„æ–™"

    prompt = f"""
    ä½ æ˜¯ä¸“ä¸šçš„é›…æ€åŠ©æ•™ã€‚è¯·ç»“åˆèƒŒæ™¯èµ„æ–™å›ç­”é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸è¶³ï¼Œè¯·ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¡¥å……ã€‚

    [èƒŒæ™¯èµ„æ–™]:
    {context}

    [é—®é¢˜]:
    {question}
    """
    response = llm.invoke(prompt)
    return {"generation": response.content, "retry_count": state.get("retry_count", 0)}


def transform_query(state):
    print("ğŸ”„ [Step 3: æ”¹å†™] å‘ç°å›ç­”è´¨é‡ä¸é«˜ï¼Œæ­£åœ¨å°è¯•æ”¹å†™é—®é¢˜...")
    question = state["question"]
    retry_count = state.get("retry_count", 0) + 1

    prompt = f"""
    ç”¨æˆ·çš„é—®é¢˜å¯èƒ½æ£€ç´¢ä¸åˆ°ç»“æœã€‚è¯·å°†å…¶æ”¹å†™ä¸ºä¸€ä¸ªæ›´å¥½çš„é›…æ€æœç´¢æŸ¥è¯¢ã€‚åªè¾“å‡ºæ–°é—®é¢˜ã€‚
    åŸé—®é¢˜: {question}
    """
    better_question = llm.invoke(prompt).content.strip()
    print(f"   âœ¨ æ–°é—®é¢˜: {better_question}")
    return {"question": better_question, "retry_count": retry_count}


def hallucination_check(state):
    print("ğŸ§  [Step 4: åæ€] æ£€æŸ¥æ˜¯å¦æœ‰å¹»è§‰...")
    generation = state["generation"]
    retry_count = state["retry_count"]

    if retry_count > 1:  # é™ä½ä¸€ç‚¹é˜ˆå€¼ï¼Œé˜²æ­¢å¤ªæ…¢
        print("   âš ï¸ é‡è¯•æ¬¡æ•°è€—å°½ï¼Œå¼ºåˆ¶è¾“å‡ºã€‚")
        return "useful"

    # è¿™é‡Œçš„ Prompt å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    prompt = f"""
    Review the answer. Does it answer the question helpfuly? Answer 'yes' or 'no'.
    Question: {state['question']}
    Answer: {generation}
    """
    grade = llm.invoke(prompt).content.lower()

    if "yes" in grade:
        print("   âœ… é€šè¿‡æ£€æŸ¥")
        return "useful"
    else:
        print("   âŒ æœªé€šè¿‡ï¼Œéœ€è¦é‡è¯•")
        return "not supported"


# === æ„å»ºå›¾ (Graph) ===
workflow = StateGraph(GraphState)

workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)
workflow.add_node("transform_query", transform_query)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_conditional_edges(
    "generate",
    hallucination_check,
    {
        "useful": END,
        "not supported": "transform_query"
    }
)
workflow.add_edge("transform_query", "retrieve")

app = workflow.compile()

# === è¿è¡Œ ===
if __name__ == "__main__":
    print("\nâœ… ç³»ç»Ÿå¯åŠ¨å®Œæ¯•ï¼è¿™æ˜¯å¸¦æœ‰ã€åæ€èƒ½åŠ›ã€‘çš„é›…æ€é«˜çº§ Agentã€‚")
    while True:
        try:
            q = input("\nğŸ™‹ è¯·æé—® (qé€€å‡º): ")
            if q.lower() in ['q', 'exit']: break

            inputs = {"question": q, "retry_count": 0}

            # ä½¿ç”¨ invoke ç›´æ¥è·å–æœ€ç»ˆç»“æœ
            result = app.invoke(inputs)
            print(f"\nğŸ¤– æœ€ç»ˆå›ç­”:\n{result['generation']}")
            print("-" * 50)

        except Exception as e:
            print(f"âŒ è¿è¡Œå‡ºé”™: {e}")