from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate

# ================= é…ç½®åŒºåŸŸ =================
DB_PATH = "./vector_db"
EMBEDDING_MODEL = "moka-ai/m3e-base"
LLM_MODEL = "qwen2.5:7b"


def rag_chat_system():
    print("â³ 1. è¿æ¥å‘é‡æ•°æ®åº“...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

    print(f"ğŸ”— 2. è¿æ¥æœ¬åœ° Ollama ({LLM_MODEL})...")
    llm = ChatOllama(model=LLM_MODEL, temperature=0.7)  # æ¸©åº¦è°ƒé«˜ä¸€ç‚¹(0.7)ï¼Œè®©å®ƒæ›´çµæ´»

    print("âœ… é›…æ€å…¨èƒ½åŠ©æ‰‹å·²å°±ç»ªï¼(æ—¢æ‡‚çŸ¥è¯†åº“ï¼Œåˆæ‡‚é€šç”¨çŸ¥è¯†)")

    while True:
        query = input("\nğŸ™‹ è¯·æé—® (exité€€å‡º): ")
        if query.lower() in ["exit", "quit", "q"]:
            break

        # --- æ£€ç´¢ç¯èŠ‚ ---
        # æ‰¾3ä¸ªç›¸å…³ç‰‡æ®µ
        docs = vectordb.similarity_search(query, k=3)

        # å³ä½¿æ²¡æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä¹Ÿä¸è¦è®© context ä¸ºç©ºï¼Œç»™å®ƒä¸€ä¸ªå ä½ç¬¦
        if not docs:
            context = "ï¼ˆæœªæ£€ç´¢åˆ°ç›¸å…³èµ„æ–™ï¼‰"
        else:
            context = "\n".join([f"- {doc.page_content}" for doc in docs])

        # --- ç”Ÿæˆç¯èŠ‚ (æ ¸å¿ƒä¿®æ”¹ï¼šæ··åˆæ¨¡å¼ Prompt) ---
        # è¿™é‡Œçš„ Prompt èµ‹äºˆäº†æ¨¡å‹â€œè‡ªä¸»è£å†³æƒâ€
        prompt_template = f"""
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é›…æ€åŠ©æ•™å’Œè‹±è¯­ä¸“å®¶ã€‚

        æˆ‘ä¸ºä½ æä¾›äº†ä¸€äº›ã€å‚è€ƒèµ„æ–™ã€‘ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹ç­–ç•¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

        1. **ä¼˜å…ˆå‚è€ƒ**ï¼šå¦‚æœã€å‚è€ƒèµ„æ–™ã€‘ä¸ç”¨æˆ·çš„é—®é¢˜**ç›´æ¥ç›¸å…³**ï¼Œè¯·åŸºäºèµ„æ–™å›ç­”ï¼Œç¡®ä¿å‡†ç¡®æ€§ã€‚
        2. **è‡ªä¸»å›ç­”**ï¼šå¦‚æœã€å‚è€ƒèµ„æ–™ã€‘æ˜¯æ— å…³çš„ã€ä¹±ç ã€æˆ–è€…å®Œå…¨æ²¡æåˆ°ç­”æ¡ˆï¼Œè¯·**å¿½ç•¥èµ„æ–™**ï¼Œç›´æ¥ä½¿ç”¨ä½ è‡ªå·±çš„ä¸“ä¸šçŸ¥è¯†æ¥å›ç­”ã€‚
        3. **ä¸è¦æ­»æ¿**ï¼šä¸è¦åœ¨å›ç­”ä¸­è¯´â€œæ ¹æ®å‚è€ƒèµ„æ–™...â€æˆ–è€…â€œèµ„æ–™é‡Œæ²¡æåˆ°...â€ï¼Œç›´æ¥ç»™å‡ºæœ€ä½³ç­”æ¡ˆå³å¯ã€‚

        ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
        {context}

        ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
        {query}

        ã€ä½ çš„å›ç­”ã€‘ï¼š
        """

        print("ğŸ¤– æ€è€ƒä¸­...")
        response = llm.invoke(prompt_template)

        print(f"\n{response.content}")


if __name__ == "__main__":
    rag_chat_system()