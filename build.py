import os
import shutil
import torch
import gc
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
DATA_PATH = "./data"  # åªæ”¾ PDF æ–‡ä»¶
DB_PATH = "./vector_db"  # æ•°æ®åº“è·¯å¾„
EMBEDDING_MODEL = "moka-ai/m3e-base"

# æ˜¾å­˜ä¿æŠ¤ï¼šæ¯æ¬¡å†™å…¥ 1000 æ¡ï¼Œé˜²æ­¢ 3060 çˆ†æ˜¾å­˜
BATCH_SIZE = 1000
# åƒåœ¾è¿‡æ»¤ï¼šå¦‚æœä¸€é¡µè¯»å‡ºæ¥çš„å­—å°‘äº 20 ä¸ªï¼Œè§†ä¸ºæ— æ•ˆé¡µï¼ˆå¯èƒ½æ˜¯å°é¢å›¾ã€ç›®å½•æˆ–æ‰«æé¡µï¼‰ï¼Œè·³è¿‡
MIN_CHAR_LIMIT = 20


def load_pdf_pure(file_path):
    """
    åªè¯»å– PDF ä¸­çš„å¯å¤åˆ¶æ–‡æœ¬ã€‚
    è¿”å›: (æœ‰æ•ˆé¡µé¢åˆ—è¡¨, çŠ¶æ€æè¿°)
    """
    try:
        loader = PyPDFLoader(file_path)
        pages = loader.load()
        valid_pages = []

        for p in pages:
            # æ¸…æ´—ï¼šå»é™¤å¤šä½™ç©ºç™½ç¬¦
            content = p.page_content.strip()
            # åªæœ‰å­—æ•°è¶…è¿‡é˜ˆå€¼çš„é¡µæ‰ç®—æœ‰æ•ˆ
            if len(content) > MIN_CHAR_LIMIT:
                valid_pages.append(p)

        if not valid_pages:
            return [], "Skipped (No Text)"

        return valid_pages, "Loaded"
    except Exception as e:
        return [], f"Error: {str(e)}"


def create_vector_db():
    # 1. ç¯å¢ƒå‡†å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âš™ï¸  è¿è¡Œè®¾å¤‡: {device.upper()}")
    if device == "cuda":
        print(f"   - æ˜¾å¡: {torch.cuda.get_device_name(0)}")

    # æ¸…ç†æ—§åº“ï¼Œä¿è¯çº¯å‡€
    if os.path.exists(DB_PATH):
        shutil.rmtree(DB_PATH)

    # 2. åŠ è½½ PDF
    print("ğŸš€ 1. å¼€å§‹è¯»å– PDF (çº¯æ–‡æœ¬æ¨¡å¼)...")
    if not os.path.exists(DATA_PATH):
        os.makedirs(DATA_PATH)
        print(f"âŒ '{DATA_PATH}' æ–‡ä»¶å¤¹ä¸å­˜åœ¨ã€‚è¯·åˆ›å»ºå¹¶æ”¾å…¥ PDFã€‚")
        return

    all_docs = []
    # åªçœ‹ .pdf æ–‡ä»¶
    files = [f for f in os.listdir(DATA_PATH) if f.lower().endswith('.pdf')]

    if not files:
        print("âš ï¸  data æ–‡ä»¶å¤¹é‡Œæ²¡æœ‰ PDF æ–‡ä»¶ã€‚")
        return

    # éå†æ–‡ä»¶
    for filename in tqdm(files, desc="è§£æè¿›åº¦"):
        file_path = os.path.join(DATA_PATH, filename)

        docs, status = load_pdf_pure(file_path)

        # çŠ¶æ€åé¦ˆ
        if status.startswith("Skipped"):
            print(f"   âš ï¸ è·³è¿‡ {filename}: çº¯å›¾ç‰‡/æ‰«æä»¶/å­—æ•°å¤ªå°‘")
        elif status.startswith("Error"):
            print(f"   âŒ é”™è¯¯ {filename}: {status}")

        if docs:
            # æ³¨å…¥æºæ–‡ä»¶åï¼ŒRAG å¿…å¤‡
            for d in docs:
                d.metadata['source'] = filename
            all_docs.extend(docs)

    print(f"\nğŸ“„ æœ‰æ•ˆæ–‡æœ¬ç‰‡æ®µ: {len(all_docs)}")
    if not all_docs:
        print("âŒ æœªæå–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼Œè¯·æ£€æŸ¥ PDF æ˜¯å¦ä¸ºæ–‡å­—ç‰ˆã€‚")
        return

    # 3. åˆ‡åˆ†æ–‡æœ¬
    print("âœ‚ï¸ 2. æ–‡æœ¬åˆ‡åˆ†...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(all_docs)
    print(f"   - åˆ‡åˆ†å‡º {len(chunks)} ä¸ªå—")

    # 4. å‘é‡åŒ–å†™å…¥ (GPU åŠ é€Ÿ)
    print(f"ğŸ§  3. åŠ è½½ Embedding æ¨¡å‹ ({device})...")

    if device == "cuda":
        torch.cuda.empty_cache()
        gc.collect()

    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )

    print(f"ğŸ’¾ 4. æ­£åœ¨å†™å…¥å‘é‡åº“ (åˆ†æ‰¹å¤„ç† BATCH={BATCH_SIZE})...")

    # åˆå§‹åŒ–æ•°æ®åº“
    vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

    # åˆ†æ‰¹å†™å…¥ï¼Œæ˜¾å­˜æ— å‹åŠ›
    for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc="å†™å…¥è¿›åº¦"):
        batch = chunks[i: i + BATCH_SIZE]
        vectordb.add_documents(batch)

    print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼è·¯å¾„: {DB_PATH}")
    print("   (ç°åœ¨å¯ä»¥å»è¿è¡Œ RAG å¯¹è¯è„šæœ¬äº†)")


if __name__ == "__main__":
    create_vector_db()