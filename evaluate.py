import os
import json
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import HuggingFaceEmbeddings

# å¯¼å…¥ä½ çš„ RAG ç³»ç»Ÿ (éœ€è¦ç¨å¾®ä¿®æ”¹ agent_rag.py ä»¥ä¾¿è°ƒç”¨ï¼Œæˆ–è€…åœ¨è¿™é‡Œé‡æ–°å®šä¹‰ä¸€éç®€å•çš„é“¾)
# ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥å®ä¾‹åŒ–ä¸€ä¸ªç®€å•çš„ RAG é“¾ï¼Œå¤ç”¨ä½ ä¹‹å‰çš„é€»è¾‘
from langchain_community.vectorstores import Chroma
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ================= é…ç½® =================
DB_PATH = "./vector_db"
EMBEDDING_MODEL = "moka-ai/m3e-base"
LLM_MODEL = "qwen2.5:7b"

print("âš™ï¸  1. åˆå§‹åŒ–è¯„ä¼°ç¯å¢ƒ...")

# 1. å‡†å¤‡ Embeddings
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

# 2. å‡†å¤‡ LLM (ä½œä¸º RAG çš„ç”Ÿæˆå™¨)
generator_llm = ChatOllama(model=LLM_MODEL, temperature=0)

# 3. å‡†å¤‡ LLM (ä½œä¸º Ragas çš„è£åˆ¤/Critic)
# Ragas é€šå¸¸æ¨èç”¨ GPT-4 å½“è£åˆ¤ï¼Œä½†ä¸ºäº†çº¯æœ¬åœ°ï¼Œæˆ‘ä»¬å¼ºåˆ¶ç”¨ Qwen å½“è£åˆ¤
critic_llm = ChatOllama(model=LLM_MODEL, temperature=0)

# 4. è¿æ¥å‘é‡åº“
vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

# 5. å®šä¹‰ç®€å•çš„ RAG é“¾ (ç”¨äºç”Ÿæˆæµ‹è¯•ç»“æœ)
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)


def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])


rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | generator_llm
        | StrOutputParser()
)

# ================= å¼€å§‹è¯„ä¼° =================
print("ğŸ“‚ 2. åŠ è½½æµ‹è¯•é›†...")
with open("test_data.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

questions = [item["question"] for item in test_data]
ground_truths = [[item["ground_truth"]] for item in test_data]  # Ragas éœ€è¦äºŒç»´åˆ—è¡¨

answers = []
contexts = []

print("ğŸ¤– 3. æ­£åœ¨ç”Ÿæˆå›ç­” (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
for i, q in enumerate(questions):
    print(f"   [{i + 1}/{len(questions)}] å¤„ç†é—®é¢˜: {q}")

    # 1. è·å–å›ç­”
    ans = rag_chain.invoke(q)
    answers.append(ans)

    # 2. è·å–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ (ä¸ºäº†è®¡ç®— Context Metrics)
    docs = retriever.invoke(q)
    ctx = [d.page_content for d in docs]
    contexts.append(ctx)

# æ„å»º Ragas æ•°æ®é›†
data_dict = {
    "question": questions,
    "answer": answers,
    "contexts": contexts,
    "ground_truth": ground_truths
}
dataset = Dataset.from_dict(data_dict)

print("âš–ï¸  4. å¼€å§‹æ‰“åˆ† (ä½¿ç”¨ Ragas)...")
# è¿™é‡Œçš„ metrics å°±æ˜¯ä½ æƒ³å±•ç¤ºçš„æ ¸å¿ƒæŒ‡æ ‡
results = evaluate(
    dataset=dataset,
    metrics=[
        faithfulness,  # å¿ å®åº¦ï¼šå›ç­”æ˜¯å¦ç”±ä¸Šä¸‹æ–‡æ”¯æ’‘ (é˜²å¹»è§‰)
        answer_relevancy,  # ç›¸å…³æ€§ï¼šå›ç­”æ˜¯å¦åˆ‡é¢˜
        # context_precision, # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼šæ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦ç›¸å…³ (å¯é€‰)
    ],
    llm=critic_llm,  # è¿™é‡Œçš„ llm æ˜¯ç”¨æ¥å½“è£åˆ¤çš„
    embeddings=embeddings  # è¿™é‡Œçš„ embeddings ç”¨æ¥è®¡ç®—ç›¸ä¼¼åº¦
)

# ================= è¾“å‡ºç»“æœ =================
print("\nğŸ“Š è¯„ä¼°æŠ¥å‘Š:")
df = results.to_pandas()
print(df[["question", "faithfulness", "answer_relevancy"]])

print("\nğŸ† å¹³å‡åˆ†:")
print(results)

# ä¿å­˜ä¸º CSV æ–¹ä¾¿æ”¾åˆ° GitHub
df.to_csv("evaluation_results.csv", index=False)
print("âœ… ç»“æœå·²ä¿å­˜è‡³ evaluation_results.csv")